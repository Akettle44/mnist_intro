There is a difference between sklearns train test split and keras validation_split
	Keras is split based on index and sklearn is random

Conv nets typically like data between 0 and 1, which means features should be normalized 
	For grayscale (8-bit), this would mean sample = sample / 255;

Categorical labels are often converted to one hot encoding if they don't depend on eachother 
	For example: if there are 10 labels, 3 could be: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
	But for other problems it doesn't have to be index based, they just need to be unique

Remember which functions are in place and which aren't (or look it up), reshape is not in place.

Notes on batch size:
	Batch size is typically between 32-512, too large of a batch = inability to generalize [1]
	Can try values in this section and see how they effect the model

Notes on conv layer sizes + amounts:
	More of an art, no definative way
	Suggestions are start with smaller layers and build your way up, reviewing loss

Numpy notes:
	Axis 0 = rows 
	Axis 1 = col
	Be careful, some functions ask for the axis to calculate along
		Ex: Row sum is sum across columns, so axis = 1
	argmax can be used to determine the indecies of the maximums

Attempted configurations:

unchanged:
	optimizer = stock rmsprop
	loss = categorical categorical_crossentropy
	metric = accuracy

1.) 32 Layer Conv2d, 3x3, no padding, relu, (28, 28 input) 
	Maxpool(2,2)
	64 Layer Conv2d, 3x3, no padding, relu, auto shape
	Maxpool(2,2)
	64 Layer Conv2d, 3x3, no padding, relu, auto shape 
	Maxpool(2,2)
	Flattening
	64 Layer Dense Relu
	10 Layer Dense softmax
	32 batch size
	10 epochs

	results:
	{'loss': [0.1910935789346695, 0.05411963909864426, 0.03767769783735275, 0.029166854918003082, 0.0217508003115654, 0.018715720623731613, 0.014973818324506283, 0.013147562742233276, 0.011285507120192051, 0.010343708097934723]
	 'accuracy': [0.9410364031791687, 0.9832212924957275, 0.9884313941001892, 0.9912045001983643, 0.9936694502830505, 0.9944537878036499, 0.9957423210144043, 0.9961344599723816, 0.9968907833099365, 0.997226893901825],
	 'val_loss': [0.0848434790968895, 0.0576842725276947, 0.06225908175110817, 0.05616123601794243, 0.05286434292793274, 0.06307103484869003, 0.07512201368808746, 0.06222500279545784, 0.07782766968011856, 0.08049710094928741],
	 'val_accuracy': [0.9725396633148193, 0.9820634722709656, 0.9830158948898315, 0.9839682579040527, 0.9861904978752136, 0.9847618937492371, 0.9826984405517578, 0.9833333492279053, 0.9852380752563477, 0.9858730435371399]}

	 interesting things to note:
	 	- loss drops off substantially from 1st epoch to second, not sure why the difference is so big
		- Val accuracy stops increasing around 4th epoch 
		- Val accuracy stops increasing but training accuracy does, suggests overfitting after epoch 5

2.) Same as 1 except batch size is changed to 64

	results:
	{'loss': [0.2281169444322586, 0.05770596116781235, 0.038486652076244354, 0.028401201590895653, 0.02325424924492836, 0.01668669283390045, 0.014486570842564106, 0.012094313278794289, 0.009690418839454651, 0.0076378025114536285],
	 'accuracy': [0.9283193349838257, 0.9822689294815063, 0.9881512522697449, 0.9906722903251648, 0.9929692149162292, 0.9946778416633606, 0.995658278465271, 0.9959383606910706, 0.9967226982116699, 0.9975069761276245],
	 'val_loss': [0.0737806186079979, 0.062398236244916916, 0.05378478392958641, 0.052498381584882736, 0.050567224621772766, 0.06720584630966187, 0.05315219983458519, 0.07401740550994873, 0.061668042093515396, 0.07594449818134308],
	 'val_accuracy': [0.977142870426178, 0.9804762005805969, 0.9830158948898315, 0.9855555295944214, 0.9852380752563477, 0.9833333492279053, 0.9873015880584717, 0.9834920763969421, 0.9865079522132874, 0.9839682579040527]}

	interesting things to note:
		- Model still appears to be overfitting after 5 epochs 
		- Increased batch size marginally increased validation accuracy

3.) Same as 1 except batch size is changed to 128

	results:
	{'loss': [0.31221768260002136, 0.07396166026592255, 0.046928923577070236, 0.03556663170456886, 0.026995649561285973, 0.02161179855465889, 0.018001738935709, 0.01397276483476162, 0.012133616022765636, 0.009360132738947868],
	 'accuracy': [0.9026050567626953, 0.9771148562431335, 0.9852660894393921, 0.9890196323394775, 0.9917086958885193, 0.9927451014518738, 0.9944537878036499, 0.9957423210144043, 0.9963305592536926, 0.9970588088035583],
	 'val_loss': [0.11275393515825272, 0.07146517932415009, 0.07374528050422668, 0.05869848281145096, 0.09755904972553253, 0.061589960008859634, 0.05592648312449455, 0.049666691571474075, 0.058641042560338974, 0.08637358993291855],
	 'val_accuracy': [0.9647619128227234, 0.9784126877784729, 0.9753968119621277, 0.9819047451019287, 0.9739682674407959, 0.9814285635948181, 0.9852380752563477, 0.9876190423965454, 0.9858730435371399, 0.9819047451019287]}

	notes:
		Changes are not significant enough from 2.) to justify increasing the batch size

4.) Same as 3.) except padding is added

	results:
	{'loss': [0.20006293058395386, 0.05087771639227867, 0.0345160998404026, 0.025201648473739624, 0.019240738824009895, 0.014445967972278595, 0.013170172460377216, 0.00996506866067648, 0.008079271763563156, 0.006895298603922129],
	 'accuracy': [0.937591016292572, 0.9841456413269043, 0.9888235330581665, 0.9922128915786743, 0.9941736459732056, 0.9957982897758484, 0.9960504174232483, 0.9967507123947144, 0.9973389506340027, 0.9979832172393799],
	 'val_loss': [0.08836963772773743, 0.06342032551765442, 0.055186927318573, 0.060870666056871414, 0.04745694249868393, 0.07678664475679398, 0.059205662459135056, 0.07423288375139236, 0.0686873123049736, 0.08431991189718246],
	 'val_accuracy': [0.9734920859336853, 0.9809523820877075, 0.9834920763969421, 0.9831746220588684, 0.9857142567634583, 0.9833333492279053, 0.9865079522132874, 0.9863492250442505, 0.9874603152275085, 0.9847618937492371]}

	notes:
		- Val Accuracy curve was smoother, experiencing less dips
		- Model seemed to begin to overfit around 5 epochs

5.) Same as 4.) except using 5x5 convolution

	results:
	{'loss': [0.17795662581920624, 0.04602264612913132, 0.028491737321019173, 0.021139591932296753, 0.016083015128970146, 0.010862120427191257, 0.009179661050438881, 0.009571393020451069, 0.006610939744859934, 0.006615127436816692],
	 'accuracy': [0.9424369931221008, 0.9859943985939026, 0.9910924434661865, 0.9935294389724731, 0.9954061508178711, 0.9970027804374695, 0.9973949790000916, 0.9973949790000916, 0.9979832172393799, 0.9981512427330017],
	 'val_loss': [0.06978164613246918, 0.04962119832634926, 0.05926890671253204, 0.0487317331135273, 0.04486069455742836, 0.06756297498941422, 0.050919998437166214, 0.05675911158323288, 0.07391571253538132, 0.0563991479575634],
	 'val_accuracy': [0.9785714149475098, 0.9844444394111633, 0.9853968024253845, 0.9871428608894348, 0.9877777695655823, 0.9844444394111633, 0.9882539510726929, 0.9893651008605957, 0.988095223903656, 0.99015873670578]}

	 notes:
	 	- Accuracy moderately increased
		- Changes were not different enough to 3x3 to jusify switching

Papers:
[1]: https://arxiv.org/abs/1609.04836